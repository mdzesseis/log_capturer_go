groups:
  - name: log_capturer_critical
    interval: 30s
    rules:
      # High Goroutine Count Alert
      - alert: HighGoroutineCount
        expr: go_goroutines{job="log-capturer"} > 8000
        for: 5m
        labels:
          severity: critical
          component: runtime
        annotations:
          summary: "High goroutine count detected"
          description: "Goroutine count is {{ $value }}, which exceeds the threshold of 8000. Possible goroutine leak."

      # Goroutine Warning
      - alert: GoroutineCountWarning
        expr: go_goroutines{job="log-capturer"} > 5000
        for: 5m
        labels:
          severity: warning
          component: runtime
        annotations:
          summary: "Elevated goroutine count"
          description: "Goroutine count is {{ $value }}, approaching critical threshold."

      # High Memory Usage Alert
      - alert: HighMemoryUsage
        expr: (process_resident_memory_bytes{job="log-capturer"} / node_memory_MemTotal_bytes) * 100 > 80
        for: 5m
        labels:
          severity: critical
          component: runtime
        annotations:
          summary: "High memory usage detected"
          description: "Memory usage is {{ $value }}%, exceeding 80% threshold."

      # Memory Warning
      - alert: MemoryUsageWarning
        expr: (process_resident_memory_bytes{job="log-capturer"} / node_memory_MemTotal_bytes) * 100 > 70
        for: 5m
        labels:
          severity: warning
          component: runtime
        annotations:
          summary: "Elevated memory usage"
          description: "Memory usage is {{ $value }}%, approaching critical threshold."

      # High File Descriptor Usage
      - alert: HighFileDescriptorUsage
        expr: (process_open_fds{job="log-capturer"} / process_max_fds{job="log-capturer"}) * 100 > 80
        for: 5m
        labels:
          severity: critical
          component: runtime
        annotations:
          summary: "High file descriptor usage"
          description: "File descriptor usage is {{ $value }}%, exceeding 80% threshold. May run out of file descriptors."

      # File Descriptor Warning
      - alert: FileDescriptorWarning
        expr: (process_open_fds{job="log-capturer"} / process_max_fds{job="log-capturer"}) * 100 > 70
        for: 5m
        labels:
          severity: warning
          component: runtime
        annotations:
          summary: "Elevated file descriptor usage"
          description: "File descriptor usage is {{ $value }}%, approaching critical threshold."

      # Low Disk Space
      - alert: LowDiskSpace
        expr: (node_filesystem_avail_bytes{job="node",mountpoint="/"} / node_filesystem_size_bytes{job="node",mountpoint="/"}) * 100 < 20
        for: 5m
        labels:
          severity: critical
          component: disk
        annotations:
          summary: "Low disk space"
          description: "Available disk space is {{ $value }}%, below 20% threshold. Immediate action required."

      # Disk Space Warning
      - alert: DiskSpaceWarning
        expr: (node_filesystem_avail_bytes{job="node",mountpoint="/"} / node_filesystem_size_bytes{job="node",mountpoint="/"}) * 100 < 30
        for: 5m
        labels:
          severity: warning
          component: disk
        annotations:
          summary: "Disk space running low"
          description: "Available disk space is {{ $value }}%, approaching critical threshold."

      # Circuit Breaker Open
      - alert: CircuitBreakerOpen
        expr: sum(circuit_breaker_state{job="log-capturer",state="open"}) by (sink) > 0
        for: 2m
        labels:
          severity: warning
          component: circuit_breaker
        annotations:
          summary: "Circuit breaker open for {{ $labels.sink }}"
          description: "Circuit breaker for sink {{ $labels.sink }} has been open for 2 minutes. Check sink connectivity."

      # Circuit Breaker Stuck Open
      - alert: CircuitBreakerStuckOpen
        expr: sum(circuit_breaker_state{job="log-capturer",state="open"}) by (sink) > 0
        for: 15m
        labels:
          severity: critical
          component: circuit_breaker
        annotations:
          summary: "Circuit breaker stuck open for {{ $labels.sink }}"
          description: "Circuit breaker for sink {{ $labels.sink }} has been open for 15+ minutes. Service degradation likely."

      # High Error Rate
      - alert: HighErrorRate
        expr: (rate(log_capturer_errors_total{job="log-capturer"}[5m]) / rate(log_capturer_logs_processed_total{job="log-capturer"}[5m])) > 0.01
        for: 5m
        labels:
          severity: critical
          component: processing
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }}, exceeding 1% threshold."

      # Elevated Error Rate
      - alert: ElevatedErrorRate
        expr: (rate(log_capturer_errors_total{job="log-capturer"}[5m]) / rate(log_capturer_logs_processed_total{job="log-capturer"}[5m])) > 0.005
        for: 5m
        labels:
          severity: warning
          component: processing
        annotations:
          summary: "Elevated error rate"
          description: "Error rate is {{ $value | humanizePercentage }}, approaching critical threshold."

      # High Queue Utilization
      - alert: HighQueueUtilization
        expr: (log_capturer_queue_size{job="log-capturer"} / 50000{job="log-capturer"}) * 100 > 90
        for: 5m
        labels:
          severity: critical
          component: dispatcher
        annotations:
          summary: "Dispatcher queue nearly full"
          description: "Queue utilization is {{ $value }}%, exceeding 90% threshold. Risk of dropping logs."

      # Queue Utilization Warning
      - alert: QueueUtilizationWarning
        expr: (log_capturer_queue_size{job="log-capturer"} / 50000{job="log-capturer"}) * 100 > 70
        for: 5m
        labels:
          severity: warning
          component: dispatcher
        annotations:
          summary: "High dispatcher queue utilization"
          description: "Queue utilization is {{ $value }}%, approaching critical threshold."

      # Service Down
      - alert: LogCapturerDown
        expr: up{job="log-capturer"} == 0
        for: 1m
        labels:
          severity: critical
          component: service
        annotations:
          summary: "Log Capturer service is down"
          description: "Log Capturer has been down for more than 1 minute."

      # No Logs Being Processed
      - alert: NoLogsProcessed
        expr: rate(log_capturer_logs_processed_total{job="log-capturer"}[5m]) == 0
        for: 10m
        labels:
          severity: warning
          component: processing
        annotations:
          summary: "No logs being processed"
          description: "No logs have been processed in the last 10 minutes. Check monitors and sources."

      # Low Throughput
      - alert: LowThroughput
        expr: rate(log_capturer_logs_processed_total{job="log-capturer"}[1m]) < 10
        for: 10m
        labels:
          severity: warning
          component: processing
        annotations:
          summary: "Low log processing throughput"
          description: "Throughput is {{ $value | humanize }} logs/sec, which is unusually low. Check system health."

      # DLQ Growing
      - alert: DLQGrowing
        expr: increase(log_capturer_dlq_entries_total{job="log-capturer"}[10m]) > 100
        for: 5m
        labels:
          severity: warning
          component: dlq
        annotations:
          summary: "Dead Letter Queue is growing"
          description: "DLQ has grown by {{ $value }} entries in the last 10 minutes. Check sink connectivity."

      # DLQ Critical
      - alert: DLQCritical
        expr: log_capturer_dlq_entries_total{job="log-capturer"} > 10000
        for: 5m
        labels:
          severity: critical
          component: dlq
        annotations:
          summary: "Dead Letter Queue has too many entries"
          description: "DLQ contains {{ $value }} entries. Urgent investigation required."

  - name: log_capturer_performance
    interval: 60s
    rules:
      # High CPU Usage
      - alert: HighCPUUsage
        expr: cpu_usage_percent{job="log-capturer"} > 80
        for: 10m
        labels:
          severity: warning
          component: runtime
        annotations:
          summary: "High CPU usage"
          description: "CPU usage is {{ $value }}%, exceeding 80% for 10+ minutes."

      # High GC Pause Time
      - alert: HighGCPauseTime
        expr: rate(gc_pause_duration_seconds_sum{job="log-capturer"}[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
          component: runtime
        annotations:
          summary: "High GC pause time"
          description: "GC pause time rate is {{ $value }}s, indicating memory pressure."

      # Sink Latency High
      - alert: SinkLatencyHigh
        expr: histogram_quantile(0.99, rate(sink_send_duration_seconds_bucket{job="log-capturer"}[5m])) > 5
        for: 5m
        labels:
          severity: warning
          component: sink
        annotations:
          summary: "High sink latency for {{ $labels.sink_type }}"
          description: "P99 latency for {{ $labels.sink_type }} is {{ $value }}s, exceeding 5s threshold."

      # Processing Latency High
      - alert: ProcessingLatencyHigh
        expr: histogram_quantile(0.99, rate(processing_duration_seconds_bucket{job="log-capturer"}[5m])) > 1
        for: 5m
        labels:
          severity: warning
          component: processing
        annotations:
          summary: "High processing latency"
          description: "P99 processing latency is {{ $value }}s, exceeding 1s threshold."

  - name: log_capturer_resource_leaks
    interval: 120s
    rules:
      # Goroutine Leak Detected
      - alert: GoroutineLeakSuspected
        expr: deriv(go_goroutines{job="log-capturer"}[30m]) > 10
        for: 15m
        labels:
          severity: warning
          component: leak_detection
        annotations:
          summary: "Possible goroutine leak"
          description: "Goroutine count is increasing at {{ $value }}/minute over the last 30 minutes."

      # Memory Leak Detected
      - alert: MemoryLeakSuspected
        expr: deriv(process_resident_memory_bytes{job="log-capturer"}[30m]) > 10485760
        for: 15m
        labels:
          severity: warning
          component: leak_detection
        annotations:
          summary: "Possible memory leak"
          description: "Memory usage is increasing at {{ $value | humanize }}B/minute over the last 30 minutes."

      # File Descriptor Leak
      - alert: FileDescriptorLeakSuspected
        expr: deriv(process_open_fds{job="log-capturer"}[30m]) > 5
        for: 15m
        labels:
          severity: warning
          component: leak_detection
        annotations:
          summary: "Possible file descriptor leak"
          description: "File descriptor count is increasing at {{ $value }}/minute over the last 30 minutes."

  - name: container_monitor_streams
    interval: 30s
    rules:
      # Goroutine Leak Detected (Container Monitor Specific)
      - alert: GoroutineLeakDetected
        expr: rate(log_capturer_goroutines[5m]) * 60 > 2
        for: 5m
        labels:
          severity: critical
          component: container_monitor
        annotations:
          summary: "Goroutine leak detected"
          description: "Goroutine growth rate is {{ $value | printf \"%.2f\" }}/min (threshold: 2/min). Immediate investigation required."

      # Stream Pool At Capacity
      - alert: StreamPoolAtCapacity
        expr: log_capturer_container_streams_active >= 50
        for: 2m
        labels:
          severity: warning
          component: container_monitor
        annotations:
          summary: "Stream pool at capacity"
          description: "All 50 stream slots are in use. System cannot handle additional containers."

      # File Descriptor Leak Detected
      - alert: FileDescriptorLeakDetected
        expr: rate(log_capturer_file_descriptors_open[10m]) * 600 > 50
        for: 5m
        labels:
          severity: warning
          component: container_monitor
        annotations:
          summary: "File descriptor leak detected"
          description: "FD growth rate is {{ $value | printf \"%.2f\" }}/10min (threshold: 50/10min). Check for unclosed streams."

      # Container Monitor Unhealthy
      - alert: ContainerMonitorUnhealthy
        expr: log_capturer_component_health{component_name="container_monitor"} == 0
        for: 1m
        labels:
          severity: critical
          component: container_monitor
        annotations:
          summary: "Container Monitor is unhealthy"
          description: "Component health check failed. Container log collection may be impacted."

      # No Stream Rotations
      - alert: NoStreamRotations
        expr: increase(log_capturer_container_stream_rotations_total[10m]) == 0
        for: 10m
        labels:
          severity: warning
          component: container_monitor
        annotations:
          summary: "Stream rotations not occurring"
          description: "No rotations detected in 10 minutes (expected every 5min). Check rotation mechanism."

      # High Stream Rotation Rate
      - alert: HighStreamRotationRate
        expr: rate(log_capturer_container_stream_rotations_total[5m]) * 60 > 20
        for: 5m
        labels:
          severity: warning
          component: container_monitor
        annotations:
          summary: "Excessive stream rotations"
          description: "Rotation rate is {{ $value | printf \"%.2f\" }}/min (threshold: 20/min). May indicate container churn."

      # Stream Pool Low Capacity
      - alert: StreamPoolLowCapacity
        expr: log_capturer_container_streams_active > 40
        for: 5m
        labels:
          severity: warning
          component: container_monitor
        annotations:
          summary: "Stream pool running low on capacity"
          description: "{{ $value }} of 50 stream slots in use (80% utilization). Consider scaling."

  # =============================================================================
  # TASK 11: NEW ALERT RULES - Deduplication, Timestamp Learning, Position System
  # =============================================================================

  - name: deduplication_monitoring
    interval: 60s
    rules:
      # High Duplicate Rate Alert
      - alert: HighDuplicateRate
        expr: log_capturer_deduplication_duplicate_rate * 100 > 30
        for: 10m
        labels:
          severity: warning
          component: deduplication
        annotations:
          summary: "High duplicate rate detected"
          description: "Duplicate rate is {{ $value | printf \"%.2f\" }}%, exceeding 30% threshold. Expected: 5-15% for container logs."

      # Deduplication Cache Full
      - alert: DeduplicationCacheFull
        expr: (log_capturer_deduplication_cache_size / 100000) * 100 > 95
        for: 30m
        labels:
          severity: warning
          component: deduplication
        annotations:
          summary: "Deduplication cache nearly full"
          description: "Cache utilization is {{ $value | printf \"%.2f\" }}%, exceeding 95% for 30+ minutes. Consider increasing max_cache_size."

      # Low Cache Hit Rate
      - alert: LowCacheHitRate
        expr: log_capturer_deduplication_cache_hit_rate * 100 < 50
        for: 15m
        labels:
          severity: warning
          component: deduplication
        annotations:
          summary: "Low deduplication cache hit rate"
          description: "Cache hit rate is {{ $value | printf \"%.2f\" }}%, below expected 70-90% range. May indicate high churn or low TTL."

      # High Eviction Rate
      - alert: HighEvictionRate
        expr: rate(log_capturer_deduplication_cache_evictions_total[5m]) > 100
        for: 15m
        labels:
          severity: info
          component: deduplication
        annotations:
          summary: "High cache eviction rate"
          description: "Eviction rate is {{ $value | printf \"%.2f\" }}/sec. Cache may be too small or TTL too short."

  - name: timestamp_learning
    interval: 60s
    rules:
      # High Timestamp Rejection Rate
      - alert: HighTimestampRejectionRate
        expr: rate(log_capturer_timestamp_rejection_total[5m]) > 10
        for: 10m
        labels:
          severity: warning
          component: timestamp_learning
        annotations:
          summary: "High timestamp rejection rate"
          description: "Rejecting {{ $value | printf \"%.2f\" }} logs/sec due to timestamp issues. Check file monitor start time or Loki retention."

      # Timestamp Clamping Active
      - alert: TimestampClampingActive
        expr: rate(log_capturer_timestamp_clamped_total[5m]) > 5
        for: 10m
        labels:
          severity: info
          component: timestamp_learning
        annotations:
          summary: "Timestamp clamping occurring frequently"
          description: "Clamping {{ $value | printf \"%.2f\" }} timestamps/sec. Historical logs being adjusted to acceptable range."

      # Loki Timestamp Errors
      - alert: LokiTimestampErrors
        expr: rate(log_capturer_loki_error_type_total{error_type="timestamp_too_old"}[5m]) > 1
        for: 5m
        labels:
          severity: warning
          component: timestamp_learning
          sink: loki
        annotations:
          summary: "Loki rejecting timestamps as too old"
          description: "Loki rejecting {{ $value | printf \"%.2f\" }} logs/sec for old timestamps. Timestamp learning may need tuning."

      # Max Acceptable Age Too Low
      - alert: TimestampMaxAgeTooLow
        expr: log_capturer_timestamp_max_acceptable_age_seconds < 3600
        for: 10m
        labels:
          severity: info
          component: timestamp_learning
        annotations:
          summary: "Timestamp max acceptable age is very low"
          description: "Max acceptable age is {{ $value | humanizeDuration }}, which may be too restrictive. Consider increasing."

  - name: position_system
    interval: 30s
    rules:
      # Position Corruption Detected
      - alert: PositionCorruptionDetected
        expr: increase(log_capturer_position_corruptions_detected_total[10m]) > 0
        for: 1m
        labels:
          severity: critical
          component: position_system
        annotations:
          summary: "Position file corruption detected"
          description: "{{ $value }} corruption(s) detected in last 10 minutes. Auto-recovery from checkpoint should occur."

      # Checkpoint Save Failures
      - alert: CheckpointSaveFailures
        expr: increase(log_capturer_position_checkpoint_save_failures_total[10m]) > 2
        for: 5m
        labels:
          severity: warning
          component: position_system
        annotations:
          summary: "Position checkpoint save failures"
          description: "{{ $value }} checkpoint save failures in last 10 minutes. Check disk space and permissions."

      # High Position Backpressure
      - alert: HighPositionBackpressure
        expr: log_capturer_position_backpressure > 0.8
        for: 10m
        labels:
          severity: warning
          component: position_system
        annotations:
          summary: "High position update backpressure"
          description: "Backpressure ratio is {{ $value | printf \"%.2f\" }} (80%+). Position updates lagging behind file reads."

      # Position Save Lag
      - alert: PositionSaveLag
        expr: log_capturer_position_save_lag_seconds > 10
        for: 5m
        labels:
          severity: warning
          component: position_system
        annotations:
          summary: "Position save lag exceeding threshold"
          description: "Save lag is {{ $value | humanizeDuration }}, exceeding 10s. Potential data loss window."

      # Position Validation Failures
      - alert: PositionValidationFailures
        expr: increase(log_capturer_position_validation_failures_total[10m]) > 5
        for: 5m
        labels:
          severity: warning
          component: position_system
        annotations:
          summary: "Position validation failures detected"
          description: "{{ $value }} validation failures in last 10 minutes. Position data may be inconsistent."

      # Checkpoint Too Old
      - alert: CheckpointTooOld
        expr: time() - log_capturer_position_checkpoint_last_saved_timestamp > 600
        for: 5m
        labels:
          severity: warning
          component: position_system
        annotations:
          summary: "Position checkpoint not saved recently"
          description: "Last checkpoint was {{ $value | humanizeDuration }} ago. Expected every 5 minutes."

      # Position System Memory High
      - alert: PositionSystemMemoryHigh
        expr: log_capturer_position_memory_usage_bytes > 15728640
        for: 10m
        labels:
          severity: warning
          component: position_system
        annotations:
          summary: "Position system memory usage high"
          description: "Using {{ $value | humanize }}B of memory (threshold: 15 MB). May need cleanup or limits."

  - name: file_monitor_enhanced
    interval: 60s
    rules:
      # File Monitor Retry Queue Full
      - alert: FileMonitorRetryQueueFull
        expr: log_capturer_file_monitor_retry_queue_size >= 50
        for: 5m
        labels:
          severity: warning
          component: file_monitor
        annotations:
          summary: "File monitor retry queue is full"
          description: "Retry queue at maximum capacity (50 entries). New failures will result in drops."

      # High Retry Failure Rate
      - alert: HighRetryFailureRate
        expr: rate(log_capturer_file_monitor_retry_failed_total[5m]) > 2
        for: 10m
        labels:
          severity: warning
          component: file_monitor
        annotations:
          summary: "High file monitor retry failure rate"
          description: "Retry failures at {{ $value | printf \"%.2f\" }}/sec. Check dispatcher or sink health."

      # Retries Being Dropped
      - alert: RetriesBeingDropped
        expr: rate(log_capturer_file_monitor_drops_total[5m]) > 0
        for: 5m
        labels:
          severity: critical
          component: file_monitor
        annotations:
          summary: "File monitor dropping retry entries"
          description: "Dropping {{ $value | printf \"%.2f\" }} entries/sec. DATA LOSS occurring!"

      # High Old Logs Ignored Rate
      - alert: HighOldLogsIgnoredRate
        expr: rate(log_capturer_file_monitor_old_logs_ignored_total[5m]) > 50
        for: 10m
        labels:
          severity: info
          component: file_monitor
        annotations:
          summary: "File monitor ignoring many old logs"
          description: "Ignoring {{ $value | printf \"%.2f\" }} old logs/sec. Historical files may contain old timestamps."

  - name: dlq_enhanced
    interval: 60s
    rules:
      # DLQ Reprocessing Failures
      - alert: DLQReprocessingFailures
        expr: rate(log_capturer_dlq_reprocessing_failures_total[10m]) > 0.5
        for: 10m
        labels:
          severity: warning
          component: dlq
        annotations:
          summary: "DLQ reprocessing failures occurring"
          description: "Reprocessing failures at {{ $value | printf \"%.2f\" }}/sec. Check sink availability."

      # DLQ Queue Size Critical
      - alert: DLQQueueSizeCritical
        expr: log_capturer_queue_size{component="dlq"} > 8000
        for: 5m
        labels:
          severity: critical
          component: dlq
        annotations:
          summary: "DLQ queue size critical"
          description: "DLQ queue contains {{ $value }} entries (80% of max capacity). Approaching emergency threshold."

      # DLQ Write Failures
      - alert: DLQWriteFailures
        expr: increase(log_capturer_dlq_write_failures_total[10m]) > 5
        for: 5m
        labels:
          severity: critical
          component: dlq
        annotations:
          summary: "DLQ write failures detected"
          description: "{{ $value }} write failures in last 10 minutes. DATA LOSS may occur!"
