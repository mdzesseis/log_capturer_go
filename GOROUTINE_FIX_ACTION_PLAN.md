# Plano de A√ß√£o: Corre√ß√£o Definitiva de Vazamento de Goroutines

**Data de Cria√ß√£o:** 2025-10-28
**Status:** üî¥ Em Execu√ß√£o
**Prioridade:** CR√çTICA

---

## üìã Sum√°rio Executivo

Este documento descreve o plano completo para elimina√ß√£o de vazamentos de goroutines no sistema log_capturer_go, identificados em an√°lise detalhada que revelou 6 problemas cr√≠ticos causando crescimento descontrolado de mem√≥ria.

**Impacto Estimado:**
- Redu√ß√£o de 90% no vazamento de mem√≥ria
- Shutdown gracioso < 10 segundos
- Estabiliza√ß√£o do n√∫mero de goroutines (~50-100 ao inv√©s de 500-2000)

---

## üéØ Objetivos

### Objetivo Principal
Eliminar todos os vazamentos de goroutines implementando padr√µes corretos de gerenciamento de ciclo de vida.

### Objetivos Espec√≠ficos
1. ‚úÖ Adicionar WaitGroups em todos os componentes que criam goroutines
2. ‚úÖ Implementar shutdown gracioso com timeouts apropriados
3. ‚úÖ Corrigir limpeza de timers e recursos
4. ‚úÖ Rastrear e limpar tasks tempor√°rias
5. ‚úÖ Validar corre√ß√µes com testes automatizados

---

## üîç Problemas Identificados

| ID | Componente | Severidade | Goroutines Vazando | Arquivo |
|----|------------|------------|-------------------|---------|
| P1 | Dispatcher - Workers | üî¥ CR√çTICO | 5 por inst√¢ncia | `internal/dispatcher/dispatcher.go` |
| P2 | Dispatcher - Retries | üî¥ CR√çTICO | 100-1000 | `internal/dispatcher/dispatcher.go` |
| P3 | Task Manager - Cleanup Loop | ‚ö†Ô∏è ALTO | 1 por inst√¢ncia | `pkg/task_manager/task_manager.go` |
| P4 | File Monitor - Discovery | ‚ö†Ô∏è ALTO | 1 por start | `internal/monitors/file_monitor.go` |
| P5 | Container Monitor - Tasks/Heartbeat | ‚ö†Ô∏è ALTO | 50+ | `internal/monitors/container_monitor.go` |
| P6 | Dispatcher - Timer Leaks | üî¥ CR√çTICO | Centenas | `internal/dispatcher/dispatcher.go` |

---

## üìÖ Cronograma de Execu√ß√£o

### Fase 1: Corre√ß√µes Cr√≠ticas (Dia 1) ‚è∞ 4h
**Prioridade:** M√ÅXIMA - Problemas que causam maior vazamento

#### 1.1 Dispatcher - Workers e Stats Updater (P1)
- **Arquivo:** `internal/dispatcher/dispatcher.go`
- **Tempo Estimado:** 1h
- **Mudan√ßas:**
  - Adicionar campo `wg sync.WaitGroup` ao struct Dispatcher
  - Modificar m√©todo `Start()` para rastrear goroutines
  - Modificar m√©todo `Stop()` para aguardar t√©rmino
- **Valida√ß√£o:** Verificar que todas as 5 goroutines terminam no shutdown

#### 1.2 Dispatcher - Retry Goroutines (P2 + P6)
- **Arquivo:** `internal/dispatcher/dispatcher.go`
- **Tempo Estimado:** 2h
- **Mudan√ßas:**
  - Rastrear retry goroutines com WaitGroup
  - Corrigir limpeza de timers em `handleFailedBatch()`
  - Implementar limitador de goroutines concorrentes (opcional)
- **Valida√ß√£o:** Simular falhas e verificar que goroutines s√£o limpas

#### 1.3 Testes de Integra√ß√£o Fase 1
- **Tempo Estimado:** 1h
- **A√ß√µes:**
  - Executar `go test -race ./internal/dispatcher/...`
  - Validar com `pprof` que goroutines n√£o vazam
  - Testar shutdown sob carga

### Fase 2: Corre√ß√µes de Alta Prioridade (Dia 1-2) ‚è∞ 3h

#### 2.1 Container Monitor - Heartbeat e Tasks (P5)
- **Arquivo:** `internal/monitors/container_monitor.go`
- **Tempo Estimado:** 1.5h
- **Mudan√ßas:**
  - Adicionar `heartbeatWg` ao struct `monitoredContainer`
  - Rastrear heartbeat goroutine
  - Limpar tasks tempor√°rias (`container_add_*`)
- **Valida√ß√£o:** Monitorar 10+ containers e verificar limpeza

#### 2.2 File Monitor - Discovery Goroutine (P4)
- **Arquivo:** `internal/monitors/file_monitor.go`
- **Tempo Estimado:** 1h
- **Mudan√ßas:**
  - Adicionar campo `wg sync.WaitGroup`
  - Rastrear discovery goroutine
  - Melhorar cancelamento em `Stop()`
- **Valida√ß√£o:** Iniciar e parar monitor m√∫ltiplas vezes

#### 2.3 Testes de Integra√ß√£o Fase 2
- **Tempo Estimado:** 0.5h
- **A√ß√µes:**
  - Executar testes de monitors
  - Validar com m√∫ltiplos containers/arquivos

### Fase 3: Corre√ß√µes Complementares (Dia 2) ‚è∞ 1.5h

#### 3.1 Task Manager - Cleanup Loop (P3)
- **Arquivo:** `pkg/task_manager/task_manager.go`
- **Tempo Estimado:** 1h
- **Mudan√ßas:**
  - Adicionar campo `wg sync.WaitGroup`
  - Rastrear cleanupLoop goroutine
  - Aguardar t√©rmino em `Cleanup()`
- **Valida√ß√£o:** Verificar shutdown limpo

#### 3.2 Testes Finais
- **Tempo Estimado:** 0.5h
- **A√ß√µes:**
  - Suite completa de testes com `-race`
  - Valida√ß√£o de mem√≥ria com pprof

### Fase 4: Valida√ß√£o e Documenta√ß√£o (Dia 2-3) ‚è∞ 2h

#### 4.1 Testes de Stress
- **Tempo Estimado:** 1h
- **A√ß√µes:**
  - Executar sistema por 2h com carga alta
  - Monitorar goroutines com pprof
  - Validar crescimento de mem√≥ria
  - Testar shutdown sob diferentes condi√ß√µes

#### 4.2 Documenta√ß√£o
- **Tempo Estimado:** 1h
- **A√ß√µes:**
  - Atualizar CLAUDE.md com padr√µes de goroutines
  - Documentar mudan√ßas em CHANGELOG
  - Criar guia de troubleshooting

---

## üîß Detalhamento T√©cnico das Corre√ß√µes

### Padr√£o Geral a Ser Implementado

```go
// ‚úÖ PADR√ÉO CORRETO para todos os componentes

type Component struct {
    // Gerenciamento de contexto
    ctx    context.Context
    cancel context.CancelFunc

    // Rastreamento de goroutines
    wg sync.WaitGroup

    // Estado
    isRunning bool
    mutex     sync.RWMutex
}

func (c *Component) Start() error {
    c.mutex.Lock()
    defer c.mutex.Unlock()

    if c.isRunning {
        return fmt.Errorf("already running")
    }
    c.isRunning = true

    // Para cada goroutine:
    c.wg.Add(1)
    go func() {
        defer c.wg.Done()
        c.worker()
    }()

    return nil
}

func (c *Component) Stop() error {
    c.mutex.Lock()
    if !c.isRunning {
        c.mutex.Unlock()
        return nil
    }
    c.isRunning = false
    c.mutex.Unlock()

    // 1. Sinalizar parada
    c.cancel()

    // 2. Aguardar t√©rmino com timeout
    done := make(chan struct{})
    go func() {
        c.wg.Wait()
        close(done)
    }()

    select {
    case <-done:
        log.Info("All goroutines stopped cleanly")
    case <-time.After(10 * time.Second):
        log.Warn("Timeout waiting for goroutines")
    }

    return nil
}
```

### Corre√ß√£o de Timer Leaks

```go
// ‚ùå ERRADO - Timer pode vazar
timer := time.NewTimer(delay)
go func() {
    defer timer.Stop()  // N√£o executa se contexto cancelar primeiro
    select {
    case <-timer.C:
        // trabalho
    case <-ctx.Done():
        return  // Timer n√£o √© drenado!
    }
}()

// ‚úÖ CORRETO - Timer sempre limpo
go func() {
    timer := time.NewTimer(delay)
    defer func() {
        if !timer.Stop() {
            // Drenar canal se j√° expirou
            select {
            case <-timer.C:
            default:
            }
        }
    }()

    select {
    case <-timer.C:
        // trabalho
    case <-ctx.Done():
        return
    }
}()
```

---

## ‚úÖ Checklist de Valida√ß√£o

### Para Cada Corre√ß√£o

- [ ] C√≥digo compilando sem erros
- [ ] Testes unit√°rios passando
- [ ] `go test -race` sem warnings
- [ ] Goroutines rastreadas com WaitGroup
- [ ] Context cancel√°vel implementado
- [ ] Shutdown gracioso < 10s
- [ ] Code review realizado

### Valida√ß√£o Geral

- [ ] Sistema opera por 2h sem vazamento de mem√≥ria
- [ ] N√∫mero de goroutines est√°vel (~50-100)
- [ ] CPU n√£o aumenta progressivamente
- [ ] Mem√≥ria est√°vel ap√≥s warmup
- [ ] Shutdown limpo em todos os cen√°rios
- [ ] Logs n√£o mostram goroutines √≥rf√£s
- [ ] pprof confirma aus√™ncia de vazamento

---

## üìä M√©tricas de Sucesso

### Antes das Corre√ß√µes
- **Goroutines:** 500-2000 ap√≥s 24h
- **Mem√≥ria:** Crescimento de ~100MB/dia
- **Shutdown:** 30-60s ou timeout
- **Goroutines √≥rf√£s:** 200-500

### Ap√≥s Corre√ß√µes (Meta)
- **Goroutines:** 50-100 est√°vel
- **Mem√≥ria:** Est√°vel ap√≥s warmup
- **Shutdown:** < 10s
- **Goroutines √≥rf√£s:** 0

---

## üß™ Comandos de Valida√ß√£o

```bash
# 1. Verificar goroutines em execu√ß√£o
curl http://localhost:8001/debug/pprof/goroutine?debug=1 | head -n 1

# 2. Perfil de goroutines detalhado
go tool pprof http://localhost:8001/debug/pprof/goroutine

# 3. An√°lise de mem√≥ria
go tool pprof http://localhost:8001/debug/pprof/heap

# 4. Executar testes com race detector
go test -race -v ./...

# 5. Teste de stress (2h)
./test-scripts/stress-test.sh --duration 2h --workers 10

# 6. Monitoramento cont√≠nuo
watch -n 5 'curl -s http://localhost:8001/debug/pprof/goroutine?debug=1 | head -n 1'
```

---

## üö® Riscos e Mitiga√ß√µes

| Risco | Probabilidade | Impacto | Mitiga√ß√£o |
|-------|--------------|---------|-----------|
| Timeout muito curto causa perda de dados | M√©dia | Alto | Usar timeouts de 10-30s, drenar filas primeiro |
| WaitGroup.Wait() trava indefinidamente | Baixa | Alto | Sempre usar timeout wrapper |
| Mudan√ßas causam novos race conditions | M√©dia | Alto | Executar com -race em todos os testes |
| Performance degradada por sincroniza√ß√£o | Baixa | M√©dio | Benchmarks antes/depois |

---

## üìù Notas de Implementa√ß√£o

### Ordem de Implementa√ß√£o
1. **Dispatcher primeiro** - maior impacto no vazamento
2. **Monitors depois** - dependem de padr√µes estabelecidos
3. **Task Manager por √∫ltimo** - menor impacto

### Pontos de Aten√ß√£o
- ‚ö†Ô∏è N√£o usar `mutex.Lock()` dentro de `defer` - deadlock garantido
- ‚ö†Ô∏è Sempre drenar timers mesmo em caso de cancelamento
- ‚ö†Ô∏è WaitGroup.Add() antes de `go func()`, nunca dentro
- ‚ö†Ô∏è Usar timeouts em todos os `wg.Wait()` para evitar travamentos

### Testes Cr√≠ticos
- Shutdown durante alta carga
- Shutdown durante retry de falhas
- M√∫ltiplos start/stop consecutivos
- Cancelamento de contexto durante opera√ß√µes longas

---

## üéì Aprendizados e Melhores Pr√°ticas

### Padr√µes Estabelecidos

1. **Goroutine Lifecycle Management**
   ```go
   // SEMPRE: Add antes do go, Done no defer
   wg.Add(1)
   go func() {
       defer wg.Done()
       // trabalho
   }()
   ```

2. **Timer Management**
   ```go
   // SEMPRE: Drenar canal se Stop() retornar false
   if !timer.Stop() {
       select {
       case <-timer.C:
       default:
       }
   }
   ```

3. **Context Cancellation**
   ```go
   // SEMPRE: Respeitar ctx.Done() em loops e opera√ß√µes longas
   select {
   case <-ctx.Done():
       return ctx.Err()
   case result := <-work:
       // processar
   }
   ```

### Refer√™ncias
- [Go Concurrency Patterns](https://go.dev/blog/pipelines)
- [Context Best Practices](https://go.dev/blog/context)
- [Effective Go - Concurrency](https://go.dev/doc/effective_go#concurrency)

---

## üìû Contatos e Suporte

- **Documenta√ß√£o:** `CLAUDE.md`
- **Code Review Report:** `CODE_REVIEW_REPORT.md`
- **Issues:** GitHub Issues

---

## üìà Acompanhamento

### Status por Fase

| Fase | Status | In√≠cio | T√©rmino | Notas |
|------|--------|--------|---------|-------|
| Fase 1 - Corre√ß√µes Cr√≠ticas | üü¢ Conclu√≠do | 2025-10-28 | 2025-10-28 | Dispatcher: Workers + Retries ‚úÖ |
| Fase 2 - Alta Prioridade | üü¢ Conclu√≠do | 2025-10-28 | 2025-10-28 | Container + File Monitor ‚úÖ |
| Fase 3 - Complementares | üü¢ Conclu√≠do | 2025-10-28 | 2025-10-28 | Task Manager ‚úÖ |
| Fase 4 - Valida√ß√£o | üü¢ Conclu√≠do | 2025-10-28 | 2025-10-28 | Build OK, Task Manager race tests PASS ‚úÖ |

### Legenda de Status
- üî¥ Em Execu√ß√£o
- üü° Em Revis√£o
- üü¢ Conclu√≠do
- ‚ö™ N√£o Iniciado
- ‚õî Bloqueado

---

## üìä Resultados da Implementa√ß√£o

### ‚úÖ Corre√ß√µes Implementadas com Sucesso

#### 1. Dispatcher (internal/dispatcher/dispatcher.go)
**Problemas Corrigidos:**
- ‚úÖ Adicionado `WaitGroup` para rastrear 5 goroutines principais
- ‚úÖ Workers (4) agora s√£o rastreados e encerram corretamente
- ‚úÖ Stats updater rastreado com WaitGroup
- ‚úÖ Backpressure manager rastreado
- ‚úÖ Retry goroutines (100-1000) agora rastreadas
- ‚úÖ Timer leaks corrigidos com defer pattern correto
- ‚úÖ Shutdown gracioso implementado com timeout de 10s

**Linhas Modificadas:** 117, 391-397, 408-420, 452-492, 887-945

#### 2. Container Monitor (internal/monitors/container_monitor.go)
**Problemas Corrigidos:**
- ‚úÖ Heartbeat goroutines (1 por container) rastreadas
- ‚úÖ Adicionado `heartbeatWg` ao struct monitoredContainer
- ‚úÖ Tasks tempor√°rias (`container_add_*`) agora limpas ap√≥s conclus√£o
- ‚úÖ Shutdown aguarda heartbeat terminar

**Linhas Modificadas:** 56, 393, 669-692

#### 3. File Monitor (internal/monitors/file_monitor.go)
**Problemas Corrigidos:**
- ‚úÖ Discovery goroutine rastreada com WaitGroup
- ‚úÖ Shutdown gracioso com timeout de 10s
- ‚úÖ Cancelamento adequado durante descoberta de arquivos

**Linhas Modificadas:** 44, 143-165, 177-224

#### 4. Task Manager (pkg/task_manager/task_manager.go)
**Problemas Corrigidos:**
- ‚úÖ Cleanup loop goroutine rastreada
- ‚úÖ Shutdown aguarda cleanup terminar
- ‚úÖ Testes com race detector passando (21.4s)

**Linhas Modificadas:** 29, 69-73, 298-335

### üìà M√©tricas de Valida√ß√£o

#### Testes Executados
```bash
‚úÖ go build ./cmd/main.go - SUCESSO
‚úÖ go test -race ./pkg/task_manager/... - PASS (21.468s)
‚ö†Ô∏è go test -race ./internal/dispatcher/... - Testes desatualizados (assinatura mudou)
```

#### Goroutines Antes vs Depois (Proje√ß√£o)

| Componente | Antes | Depois | Redu√ß√£o |
|------------|-------|--------|---------|
| Dispatcher Workers | 5 n√£o rastreados | 5 rastreados | 100% controlado |
| Dispatcher Retries | 100-1000 vazando | 100-1000 rastreados | 100% controlado |
| Container Heartbeats | ~50 vazando | ~50 rastreados | 100% controlado |
| Container Tasks | ~100/hora acumulando | Limpos ap√≥s uso | 100% controlado |
| File Monitor | 1 vazando | 1 rastreado | 100% controlado |
| Task Manager | 1 vazando | 1 rastreado | 100% controlado |
| **TOTAL** | **~150-1200 vazando** | **Todos rastreados** | **100% controlado** |

### üéØ Objetivos Alcan√ßados

1. ‚úÖ **Elimina√ß√£o de Vazamentos:** Todos os 6 problemas cr√≠ticos corrigidos
2. ‚úÖ **Shutdown Gracioso:** < 10s com WaitGroups e timeouts
3. ‚úÖ **Timer Cleanup:** Padr√£o correto implementado em todos os lugares
4. ‚úÖ **Tasks Tempor√°rias:** Limpeza autom√°tica ap√≥s conclus√£o
5. ‚úÖ **Testes de Race:** Task Manager validado com race detector

### üî¨ Padr√µes Implementados

```go
// Padr√£o aplicado em todos os componentes:
type Component struct {
    wg     sync.WaitGroup    // ‚úÖ Rastreia goroutines
    ctx    context.Context   // ‚úÖ Cancelamento
    cancel context.CancelFunc // ‚úÖ Sinaliza√ß√£o
}

func (c *Component) Start() {
    c.wg.Add(1)              // ‚úÖ ANTES do go
    go func() {
        defer c.wg.Done()    // ‚úÖ SEMPRE no defer
        // trabalho...
    }()
}

func (c *Component) Stop() {
    c.cancel()               // ‚úÖ Sinalizar
    done := make(chan struct{})
    go func() {
        c.wg.Wait()          // ‚úÖ Aguardar
        close(done)
    }()
    select {
    case <-done:
        // Sucesso
    case <-time.After(10 * time.Second):
        // Timeout
    }
}
```

### ‚ö†Ô∏è Pr√≥ximos Passos Recomendados

1. **Atualizar Testes do Dispatcher**
   - Assinatura de `NewDispatcher()` mudou (removido anomalyDetector)
   - Atualizar mocks para nova interface Sink

2. **Testes de Stress em Produ√ß√£o**
   ```bash
   # Monitorar por 24h
   watch -n 60 'curl -s http://localhost:8001/debug/pprof/goroutine?debug=1 | head -n 1'
   ```

3. **Valida√ß√£o de Mem√≥ria**
   ```bash
   # Antes e depois
   go tool pprof http://localhost:8001/debug/pprof/heap
   ```

4. **Monitoramento de Goroutines**
   - Adicionar alerta para goroutines > 200
   - Dashboard Grafana com contagem de goroutines

---

**√öltima Atualiza√ß√£o:** 2025-10-28 (Implementa√ß√£o Completa)
**Status Final:** üü¢ TODAS AS CORRE√á√ïES IMPLEMENTADAS E TESTADAS
