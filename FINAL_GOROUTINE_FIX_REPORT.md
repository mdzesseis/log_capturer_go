# üîç Final Goroutine Leak Investigation Report

**Data**: 2025-11-06
**Dura√ß√£o**: ~3 horas de an√°lise e fixes
**Status**: ‚ö†Ô∏è  **LEAK PERSISTE** - Causa raiz ainda n√£o resolvida

---

## üìä Executive Summary

Foram aplicadas **m√∫ltiplas fixes** para resolver o vazamento de goroutines no `log_capturer_go`, mas o leak **persiste** a uma taxa de **~31 goroutines/min**.

### M√©tricas Finais
| M√©trica | Antes | Depois Fixes | Target | Status |
|---------|-------|--------------|--------|--------|
| Goroutines | 1525 | 700+ | <100 | ‚ùå |
| Growth Rate | 36/min | 31/min | 0/min | ‚ùå |
| File Descriptors | 752/1024 (73%) | ~400/4096 (10%) | <20% | ‚úÖ |
| Loki Success Rate | 4.3% | 97.5% (80/82) | >99% | ‚úÖ |
| Timestamp Errors | Centenas | 2 | 0 | ‚úÖ |
| Container Health | UNHEALTHY | UNHEALTHY | HEALTHY | ‚ùå |

**Sucessos Parciais**:
- ‚úÖ Timezone fix reduziu rejei√ß√µes do Loki de 95% ‚Üí 2%
- ‚úÖ File descriptors melhoraram drasticamente
- ‚ùå Goroutine leak AINDA ATIVO (~14% redu√ß√£o de taxa)

---

## ‚úÖ FIXES APLICADOS

### 1. Timezone UTC Fix
**Problema**: Timestamps 3h no futuro causavam rejei√ß√£o do Loki
**Solu√ß√£o**: Alterado `time.Now()` ‚Üí `time.Now().UTC()` em 21 locais

**Arquivos**:
- `internal/monitors/container_monitor.go`
- `internal/monitors/file_monitor.go`
- `internal/dispatcher/dispatcher.go`
- `pkg/*` (diversos)

**Resultado**: ‚úÖ Rejei√ß√µes reduziram 99%+

### 2. File Descriptor Limits
**Problema**: Limite de 1024 FDs muito baixo
**Solu√ß√£o**: Aumentado para 4096 (soft) / 8192 (hard)

**Arquivo**: `docker-compose.yml`
```yaml
ulimits:
  nofile:
    soft: 4096
    hard: 8192
```

**Resultado**: ‚úÖ Utiliza√ß√£o caiu de 73% ‚Üí 10%

### 3. Loki Batch Size
**Problema**: Batches gigantes (20K) rejeitados
**Solu√ß√£o**: Reduzido para 500 entries, timeout 40s ‚Üí 5s

**Arquivo**: `configs/config.yaml`
**Resultado**: ‚úÖ Loki aceitando batches

### 4. Semaphore Pattern
**Problema**: Goroutines ilimitadas de `sendBatch`
**Solu√ß√£o**: Adicionado sem√°foro limitando a 15 concurrent

**Arquivo**: `internal/sinks/loki_sink.go`
- Campo `sendSemaphore chan struct{}`
- Acquire/release ao redor de spawns

**Resultado**: ‚ùå Sem impacto (31 ‚Üí 31/min)

### 5. Worker Pool Architecture
**Problema**: Spawn ilimitado de goroutines
**Solu√ß√£o**: Pool fixo de 10 workers processando fila de batches

**Implementa√ß√£o**:
```go
type LokiSink struct {
    batchQueue  chan []types.LogEntry  // Queue de batches
    workerCount int                     // 10 workers fixos
    workersWg   sync.WaitGroup         // Track workers
}

func (ls *LokiSink) startWorkers() {
    for i := 0; i < 10; i++ {
        go ls.worker(i)  // Fixed workers
    }
}

func (ls *LokiSink) worker(id int) {
    for batch := range ls.batchQueue {
        ls.sendBatch(batch)  // Process from queue
    }
}
```

**Refatora√ß√µes**:
- `flushBatch()`: Enfileira ao inv√©s de spawnar
- `adaptiveBatchLoop()`: Enfileira ao inv√©s de spawnar

**Resultado**: ‚ùå Sem impacto (31 ‚Üí 31/min)

---

## üîç ROOT CAUSE ANALYSIS

### Por Que as Fixes N√£o Funcionaram?

**Hip√≥tese Original** (INCORRETA):
- Pens√°vamos que goroutines eram criadas ilimitadamente
- Sem√°foro/worker pool limitariam cria√ß√£o

**Realidade Descoberta**:
- Workers s√£o LIMITADOS (10 para Loki, 6 para dispatcher, 3 para file)
- Mas goroutines AINDA crescem 31/min
- **Conclus√£o**: O problema n√£o √© CRIA√á√ÉO, √© que goroutines **n√£o est√£o terminando**

### Evid√™ncias

1. **Workers iniciando corretamente**:
```
"Starting Loki sink worker pool", "worker_count":10
"Dispatcher worker started", "worker_id":0...5
"Started local file sink workers", "worker_count":3
```

2. **Baseline alto suspeito**:
- Ap√≥s restart: ~165 goroutines (esperado: <50)
- Ap√≥s 5 min: ~700 goroutines
- **730+ goroutines** al√©m dos workers fixos!

3. **Loki funcionando bem**:
- Success rate: 97.5% (80/82 batches)
- Apenas 2 timestamp errors
- Workers processando normalmente

### üéØ Causa Raiz Prov√°vel

**As goroutines que vazam N√ÉO S√ÉO dos sinks!**

Poss√≠veis fontes:
1. **Monitor de containers**: `readContainerLogs()` spawna goroutine por container, mas pode n√£o estar terminando
2. **File monitor**: `readFile()` pode spawnar m√∫ltiplas goroutines
3. **Dispatcher**: Algum loop ou retry mechanism
4. **Circuit breaker**: Pode estar criando goroutines de monitoring
5. **Adaptive batcher**: Pode ter workers pr√≥prios
6. **Metrics/monitoring**: Coletores podem estar spawnando

---

## üî¨ PR√ìXIMAS INVESTIGA√á√ïES NECESS√ÅRIAS

### An√°lise com pprof

```bash
# Obter profile de goroutines
curl http://localhost:6060/debug/pprof/goroutine > goroutine.prof
go tool pprof -top goroutine.prof

# Ver stacks das goroutines
go tool pprof -http=:8080 goroutine.prof
```

**Procurar por**:
- Stacks com >50 goroutines do mesmo tipo
- Fun√ß√µes bloqueadas (chan receive, mutex, select)
- Goroutines em loops infinitos

### An√°lise de C√≥digo

**Verificar todos `go func()` em**:
```bash
grep -rn "go func\|go .*\.\|\.Add(1)" internal/ pkg/ --include="*.go" | grep -v "_test"
```

**Focar em**:
- Monitors (container_monitor.go, file_monitor.go)
- Adaptive batcher
- Circuit breaker
- Retry managers
- Metrics collectors

### Teste de Isolamento

1. **Desabilitar sinks um por um**:
```yaml
sinks:
  loki:
    enabled: false  # Test sem Loki
  local_file:
    enabled: false  # Test sem file sink
```

2. **Desabilitar monitors**:
```yaml
monitors:
  containers:
    enabled: false  # Test sem container monitoring
  files:
    enabled: false  # Test sem file monitoring
```

3. **Reduzir workers**:
```yaml
dispatcher:
  workers: 1  # Minimal workers
```

**Objetivo**: Identificar qual componente est√° vazando

---

## üìù ARQUITETURA ATUAL

### Goroutines Leg√≠timas (Esperadas)

| Componente | Goroutines | Nota |
|------------|------------|------|
| Loki Sink Workers | 10 | Fixed pool |
| Local File Sink Workers | 3 | Fixed pool |
| Dispatcher Workers | 6 | Fixed pool |
| Adaptive Batcher | 1-2 | Loop + timer |
| Container Monitor Heartbeats | 8 | 1 por container |
| Container Log Readers | 8 | 1 por container |
| File Monitor Readers | 6 | 1 por arquivo |
| Metrics Collectors | ~5 | Goroutine tracker, etc |
| HTTP Servers | ~10 | API + metrics |
| **Total Esperado** | **~60** | ‚úÖ Razo√°vel |
| **Realidade** | **700+** | ‚ùå **640+ leaked!** |

### Goroutines Vazadas (640+)

**Origem desconhecida**. Necess√°rio pprof para identificar.

---

## üõ†Ô∏è PR√ìXIMOS PASSOS RECOMENDADOS

### URGENTE (Pr√≥ximas Horas)

1. **Habilitar pprof** (se n√£o estiver)
```go
import _ "net/http/pprof"
```

2. **Capturar goroutine profile**
```bash
curl http://localhost:6060/debug/pprof/goroutine > /tmp/goroutines.prof
go tool pprof -top /tmp/goroutines.prof | head -30
```

3. **Identificar top stacks** com >50 goroutines

4. **Adicionar tracking detalhado** nesse c√≥digo

### CURTO PRAZO (Esta Semana)

1. **Implementar goroutine naming**
```go
go func() {
    runtime.SetGoroutineName("loki-worker-1")
    // ...
}()
```

2. **Adicionar m√©tricas de goroutines por tipo**
```go
metrics.SetGauge("goroutines_loki_workers", float64(count))
metrics.SetGauge("goroutines_monitors", float64(count))
```

3. **Teste de carga controlado**
- Monitorar com 1, 5, 10 containers
- Ver correla√ß√£o entre containers e goroutines

### M√âDIO PRAZO (Pr√≥ximo Sprint)

1. **Refatorar monitors** para usar pools como os sinks

2. **Implementar timeout agressivo** em todas goroutines
```go
ctx, cancel := context.WithTimeout(ctx, 30*time.Second)
defer cancel()
```

3. **Adicionar circuit breaker** para spawn de goroutines
```go
if currentGoroutines > threshold {
    return ErrTooManyGoroutines
}
```

---

## üí° LI√á√ïES APRENDIDAS

### O Que Funcionou

1. **Timezone fix**: Simple but effective
2. **Worker pools**: Boa arquitetura, mas aplicado no lugar errado
3. **Documenta√ß√£o**: Checkpoint permitiu continuidade
4. **Instrumenta√ß√£o**: M√©tricas ajudaram a identificar problema

### O Que N√£o Funcionou

1. **Assumption-based fixes**: Assumimos que sinks eram o problema
2. **Semaphore**: Limita cria√ß√£o, n√£o resolve goroutines travadas
3. **Falta de profiling**: Dever√≠amos ter come√ßado com pprof
4. **Teste insuficiente**: Cada fix deveria ter 30min de monitoring

### Melhores Pr√°ticas Identificadas

1. **SEMPRE come√ßar com profiling** antes de fixes
2. **Teste de isolamento** para identificar componente
3. **M√©tricas granulares** (goroutines por componente)
4. **Monitoring de longo prazo** (48h m√≠nimo)

---

## üìä DADOS HIST√ìRICOS

### Timeline de Fixes

| Hora | Fix | Goroutines (baseline ‚Üí +5min) | Taxa |
|------|-----|-------------------------------|------|
| 09:00 | Inicio | 1525 | 36/min |
| 09:30 | Timezone | 293 ‚Üí 358 | 13/min (??) |
| 10:00 | Semaphore | 165 ‚Üí 263 | 20/min |
| 10:30 | Worker Pool | 544 ‚Üí 701 | 31/min |

**Observa√ß√£o estranha**: A taxa VARIOU entre 13-36/min, sugerindo:
- Restart limpa goroutines acumuladas
- Taxa de "leak puro" pode ser ~13-20/min
- Ap√≥s certo ponto, taxa acelera (deadlock cascade?)

---

## üéØ CRIT√âRIOS DE SUCESSO

Para considerar o problema **resolvido**:

1. ‚úÖ Goroutine count < 100 ap√≥s 1h uptime
2. ‚úÖ Growth rate < 1 goroutine/min
3. ‚úÖ Container health: HEALTHY
4. ‚úÖ 48h uptime sem restart
5. ‚úÖ Memory usage est√°vel (<200MB)
6. ‚úÖ CPU usage <10%

**Status Atual**: 0/6 crit√©rios atingidos ‚ùå

---

## üìû CONTATOS E RECURSOS

**Documentos Relacionados**:
- `CHECKPOINT_GOROUTINE_FIX.md` - Progresso detalhado
- `EXECUTIVE_SUMMARY.md` - Vis√£o executiva
- `GOROUTINE_LEAK_FIX_PATCH.md` - Tentativa de fix original

**Comandos √öteis**:
```bash
# Monitor goroutines
watch -n 10 'curl -s http://localhost:8001/metrics | grep goroutines'

# Health check
curl -s http://localhost:8401/health | jq '.services.goroutine_tracker'

# Goroutine profile
curl http://localhost:6060/debug/pprof/goroutine > /tmp/g.prof
go tool pprof -top /tmp/g.prof

# Restart limpo
docker-compose restart log_capturer_go
```

---

**Conclus√£o**: O problema √© **mais profundo** do que sinks. Necess√°rio an√°lise com pprof para identificar fonte real do leak.

**Pr√≥ximo Passo**: Capturar e analisar goroutine profile com pprof.

**√öltima Atualiza√ß√£o**: 2025-11-06 15:35 UTC
